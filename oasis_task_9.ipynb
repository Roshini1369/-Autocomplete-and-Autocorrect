{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0FtQoM96yxqTiZJnZExLZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roshini1369/-Autocomplete-and-Autocorrect/blob/main/oasis_task_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3wCOAeg3TFKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fb1ace-4315-467b-cd86-e624d2a9f434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset (e.g., a corpus of text messages, books, tweets, etc.)\n",
        "# Sample dataset: Gutenberg Corpus from NLTK\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "raw_text = gutenberg.raw('austen-emma.txt')\n",
        "print(raw_text[:500])  # preview\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = clean_text.split()  # basic tokenizer (just splits on spaces)\n",
        "print(tokens[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqIJXILEwhnd",
        "outputId": "baca6351-cce3-4b8b-dd50-5b51eb454082"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_ngram_model(tokens, n=3):\n",
        "    model = defaultdict(list)\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        key = tuple(tokens[i:i + n - 1])\n",
        "        next_word = tokens[i + n - 1]\n",
        "        model[key].append(next_word)\n",
        "    return model\n",
        "\n",
        "# Build trigram model\n",
        "trigram_model = build_ngram_model(tokens, n=3)\n"
      ],
      "metadata": {
        "id": "PcH6oKDB2p9G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autocomplete(prompt, model, n=3):\n",
        "    prompt_tokens = prompt.lower().split()\n",
        "    if len(prompt_tokens) < n - 1:\n",
        "        return []\n",
        "\n",
        "    key = tuple(prompt_tokens[-(n - 1):])\n",
        "    return model.get(key, [])\n"
      ],
      "metadata": {
        "id": "E40UNSqQ2y48"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try some predictions\n",
        "predictions = autocomplete(\"she was\", trigram_model)\n",
        "print(\"Predicted next words:\", predictions[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uYdLxis21Ek",
        "outputId": "00454738-c639-46f0-a33d-3c13e19acaed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next words: ['the', 'now', 'more', 'a', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def autocomplete_best(prompt, model, n=3):\n",
        "    prompt_tokens = prompt.lower().split()\n",
        "    if len(prompt_tokens) < n - 1:\n",
        "        return None\n",
        "\n",
        "    key = tuple(prompt_tokens[-(n - 1):])\n",
        "    possible_next = model.get(key, [])\n",
        "    if not possible_next:\n",
        "        return None\n",
        "\n",
        "    # Return most common predicted word\n",
        "    return Counter(possible_next).most_common(1)[0][0]\n",
        "\n",
        "# Try best guess\n",
        "best_word = autocomplete_best(\"she was\", trigram_model)\n",
        "print(\"Best predicted next word:\", best_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMF-Ij-c23FY",
        "outputId": "59a89c59-59c4-4bf1-a1ce-d41705fea5da"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best predicted next word: not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textdistance\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeOyWL5i25sv",
        "outputId": "cc276544-c5bf-4f12-8dca-02fceb25809a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textdistance\n",
            "  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Downloading textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textdistance\n",
        "\n",
        "# Build a vocabulary from your tokens\n",
        "vocabulary = set(tokens)\n",
        "\n",
        "def autocorrect(word, vocab, max_distance=2):\n",
        "    suggestions = [w for w in vocab if textdistance.levenshtein(word, w) <= max_distance]\n",
        "    suggestions = sorted(suggestions, key=lambda w: textdistance.levenshtein(word, w))\n",
        "    return suggestions[:5]  # return top 5 suggestions\n",
        "\n",
        "# Example\n",
        "misspelled = \"definately\"\n",
        "corrections = autocorrect(misspelled, vocabulary)\n",
        "print(\"Suggestions:\", corrections)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a-HTeR43Cdg",
        "outputId": "6d91941f-a2ef-43fb-c4ee-61ae104812f2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Suggestions: ['delicately']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def smart_typing_assist(prompt, model, vocab, n=3):\n",
        "    words = prompt.lower().split()\n",
        "    if words:\n",
        "        last_word = words[-1]\n",
        "        corrections = autocorrect(last_word, vocab)\n",
        "        if corrections:\n",
        "            words[-1] = corrections[0]  # Use the best autocorrection\n",
        "\n",
        "    # Now predict next word\n",
        "    new_prompt = ' '.join(words)\n",
        "    next_word = autocomplete_best(new_prompt, model, n)\n",
        "    return corrections[:3], next_word\n"
      ],
      "metadata": {
        "id": "7CiKg0u83FQR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"she was definately\"\n",
        "corrections, prediction = smart_typing_assist(prompt, trigram_model, vocabulary)\n",
        "print(\"Autocorrect suggestions:\", corrections)\n",
        "print(\"Predicted next word:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F8aCO5B3H9R",
        "outputId": "642ba906-7d34-40c8-88ca-c7b42648f78f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autocorrect suggestions: ['delicately']\n",
            "Predicted next word: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2gCjkis23LTN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}